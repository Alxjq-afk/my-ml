# JARVIS Advanced v2.0

Asistente de voz tipo Cortana/Alexa con escucha continua, STT, LLM e integraciÃ³n de APIs.

## ğŸ¯ CaracterÃ­sticas

### Backend LLM
- **Mistral 7B** local (4.37 GB GGUF) - cuando llama-cpp-python compile en tu sistema
- **Hugging Face Inference API** - fallback remoto (configurado por defecto)
- Respuestas inteligentes contextuales si ambos fallan

### Speech-to-Text (STT)
- **OpenAI Whisper** con soporte espaÃ±ol
- Modelos: tiny, base (default), small, medium
- TranscripciÃ³n en tiempo real desde micrÃ³fono

### Wake Word Detection
- Detecta "Hey JARVIS" para activar escucha
- Google Speech Recognition como base (requiere internet)
- PocketSphinx como alternativa local (sin internet)

### IntÃ©rpretes & Comandos
- **CommandInterpreter**: Comandos naturales sin prefijo (ej: "abre notepad")
- EjecuciÃ³n automÃ¡tica de: exec, open, volume, sendmail
- DetecciÃ³n de intenciones en espaÃ±ol

### APIs Integradas
- â° Hora, fecha, zona horaria
- ğŸŒ BÃºsqueda web (DuckDuckGo, sin API key)
- ğŸ“Š Info del sistema (CPU, memoria, disco)
- ğŸ§® CÃ¡lculos matemÃ¡ticos seguros

### Text-to-Speech (TTS)
- pyttsx3 (local, sin calidad)
- Habla respuestas automÃ¡ticamente

### Memoria
- Conversaciones guardadas en JSON local
- Historial persistente

## ğŸ“¦ InstalaciÃ³n

### 1. Dependencias base
```bash
pip install -r requirements.txt
```

### 2. Dependencias de voz
Ya instaladas:
```bash
pip install openai-whisper pyaudio scipy librosa speech-recognition pocketsphinx
pip install sounddevice requests python-dotenv psutil
```

### 3. Mistral 7B (opcional, pero recomendado)
Archivo ya descargado en: `C:\Users\anune\models\mistral-7b-instruct-v0.1.Q4_K_M.gguf`

Para usar localmente, compilar **llama-cpp-python**:
```bash
# Requiere Visual Studio Build Tools en Windows
pip install llama-cpp-python
```

### 4. Token Hugging Face
Agregado en `.env`:
```
REMOTE_PROVIDER=hf
HF_API_KEY=<tu_token_aqui>  # Registrate en huggingface.co
REMOTE_MODEL=distilgpt2
```

> âš ï¸ **Importante**: Nunca compartas tu token HF pÃºblicamente. Es una credencial sensible.

## ğŸ¤ Modos de ejecuciÃ³n

### CLI Mode (Texto)
```bash
python run_jarvis_voice.py --mode cli
```
Escribe comandos como en `run_assistant.py`

### Voice Mode (Voz)
```bash
python run_jarvis_voice.py --mode voice
```
- Escucha continua
- Di "Hey JARVIS" para activar
- Responde por voz

### Hybrid Mode (Auto, default)
```bash
python run_jarvis_voice.py
```
- CLI por defecto
- ActivaciÃ³n por voz detecta comandos

## âš™ï¸ Opciones

```bash
python run_jarvis_voice.py \
  --mode hybrid \                 # cli, voice, hybrid
  --confirm-actions \             # Pedir confirmaciÃ³n
  --no-tts \                      # Desabilitar sÃ­ntesis de voz
  --stt-model small               # tiny, base, small, medium
```

## ğŸ“ Comandos Naturales

Sin necesidad de prefijos:

```
"abre notepad"              â†’ Abre Notepad
"ejecuta dir C:\"           â†’ Ejecuta comando dir
"sube volumen a 70"         â†’ Ajusta volumen a 70%
"baja volumen"              â†’ Baja volumen 10%
"envÃ­a un correo"           â†’ Inicia envÃ­o de email
"Â¿QuÃ© hora es?"             â†’ Pregunta a JARVIS
"Â¿CuÃ¡l es la capital...?"   â†’ ConversaciÃ³n normal
```

## ğŸ—ï¸ Arquitectura

```
assistant/
  â”œâ”€â”€ config.py         # Carga .env
  â”œâ”€â”€ llm.py            # LocalLLM (Mistral) + RemoteLLM (HF)
  â”œâ”€â”€ memory.py         # Historial JSON
  â”œâ”€â”€ executor.py       # Ejecuta comandos
  â”œâ”€â”€ voice.py          # TTS (pyttsx3)
  â”œâ”€â”€ interpreter.py    # CommandInterpreter (natural language)
  â”œâ”€â”€ stt.py            # WhisperSTT (Speech-to-Text)
  â”œâ”€â”€ wake_word.py      # WakeWordDetector ("Hey JARVIS")
  â””â”€â”€ apis.py           # Hora, bÃºsqueda, clima, etc.

run_jarvis_voice.py       # CLI principal con voz
test_apis.py              # Tests de APIs
test_mistral.py           # Tests de LLM
```

## ğŸ”§ Troubleshooting

### "llama-cpp-python no disponible"
- Requiere compilaciÃ³n con Visual Studio Build Tools
- Por ahora, JARVIS usa Hugging Face como fallback
- Token HF ya configurado en `.env`

### No se escucha micrÃ³fono
```bash
# Verificar dispositivos de audio
python -c "import sounddevice as sd; print(sd.query_devices())"
```

### Whisper tarda mucho
- Usa `--stt-model tiny` para inferencia mÃ¡s rÃ¡pida
- O descarga modelo mÃ¡s pequeÃ±o

### Google Speech Recognition falla
- Necesita conexiÃ³n a internet
- Intenta con micrÃ³fono mÃ¡s cercano
- Verifica que no haya bloqueadores

## ğŸ“Š Benchmarks

- **Whisper (base)**: ~5-10 segundos por grabaciÃ³n (CPU)
- **Mistral 7B (local)**: ~2-5 tokens/seg (CPU-only, lento)
- **Hugging Face API**: ~0.5-1 segundo (remoto, rÃ¡pido)
- **Total flujo (voz)**: ~15-20 segundos end-to-end

## ğŸš€ Mejoras futuras

- [ ] Compilar llama-cpp-python con CUDA para GPU
- [ ] Usar Ollama para Mistral local mÃ¡s rÃ¡pido
- [ ] Integrar Cortana/Windows Speech Recognition nativa
- [ ] Agregar smart home integration (Philips Hue, etc.)
- [ ] Machine learning para detecciÃ³n de intenciones
- [ ] Integrar con calendarios y email
- [ ] Soporte para mÃºltiples idiomas

## ğŸ“„ Licencia

Mismo proyecto que `my-ml` - Educational

---

**VersiÃ³n**: 2.0 (Advanced Voice)  
**Fecha**: Noviembre 2025  
**Autor**: Tu nombre
