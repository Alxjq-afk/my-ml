"""Wrapper para modelo local (llama_cpp) con fallback remoto (Hugging Face).

Intenta cargar `llama_cpp` si est√° disponible y `MODEL_PATH` en `.env`.
Si no hay modelo local, intenta usar RemoteLLM (Hugging Face API) si est√° configurado.
Si tampoco, devuelve respuestas de fallback instructivas.
"""
from typing import Optional
import os
import json
import requests


class RemoteLLM:
    """Cliente m√≠nimo para Hugging Face Inference API.

    Usa las variables de entorno:
    - REMOTE_PROVIDER: 'hf' para Hugging Face (por ahora √∫nico soportado)
    - HF_API_KEY: token de Hugging Face
    - REMOTE_MODEL: id del modelo en HF (p.ej. 'meta-llama/Llama-2-7b-chat')
    """

    def __init__(self):
        self.provider = os.getenv("REMOTE_PROVIDER", "hf")
        self.api_key = os.getenv("HF_API_KEY")
        self.model = os.getenv("REMOTE_MODEL")

    def generate(self, prompt: str, max_tokens: int = 256) -> str:
        if self.provider != "hf":
            return "[RemoteLLM] proveedor remoto no configurado (solo 'hf' soportado)."

        if not self.api_key or not self.model:
            return "[RemoteLLM] falta HF_API_KEY o REMOTE_MODEL en el entorno."

        url = f"https://api-inference.huggingface.co/models/{self.model}"
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"inputs": prompt, "parameters": {"max_new_tokens": max_tokens}}
        try:
            resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
            resp.raise_for_status()
            data = resp.json()
            # Manejar formatos: {'generated_text': '...'} o list of dicts
            if isinstance(data, dict) and "generated_text" in data:
                return data["generated_text"].strip()
            if isinstance(data, list) and len(data) > 0 and "generated_text" in data[0]:
                return data[0]["generated_text"].strip()
            # Algunos endpoints devuelven texto plano
            if isinstance(data, str):
                return data.strip()
            return str(data)
        except Exception as e:
            return f"[RemoteLLM error] {e}"


class LocalLLM:
    """Interfaz unificada: intenta usar un backend local (`llama_cpp`) y si no est√°
    disponible intenta un backend remoto configurado, o devuelve respuestas de fallback.
    
    Soporta:
    - Backend local: llama-cpp-python con Mistral 7B GGUF (~4GB)
    - Backend remoto: Hugging Face Inference API
    - Fallback: respuestas inteligentes contextuales
    """

    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path or os.getenv("MODEL_PATH")
        self.backend = None
        self.available = False
        self.llm = None

        # Intentar backend local (llama_cpp)
        try:
            from llama_cpp import Llama

            if self.model_path and os.path.exists(self.model_path):
                print(f"üì¶ Cargando modelo Mistral 7B desde {self.model_path}")
                self.llm = Llama(
                    model_path=self.model_path,
                    n_gpu_layers=0,  # CPU only (cambiar a GPU si tienes CUDA)
                    n_ctx=2048,      # Contexto
                    verbose=False
                )
                self.available = True
                self.backend = "llama_cpp"
                print("‚úì Modelo Mistral 7B cargado exitosamente")
            else:
                print(f"‚ö† Modelo no encontrado: {self.model_path}")
                self.llm = None
        except ImportError:
            print("‚ö† llama-cpp-python no disponible. Instalalo con: pip install llama-cpp-python")
        except Exception as e:
            print(f"‚ö† Error cargando modelo local: {e}")
            self.llm = None

        # Si no hay backend local, intentar remoto si est√° configurado
        if not self.available:
            provider = os.getenv("REMOTE_PROVIDER")
            if provider:
                print("üåê Usando backend remoto (Hugging Face)")
                self.llm = RemoteLLM()
                self.backend = "remote"
                self.available = True

    def generate(self, prompt: str, max_tokens: int = 256) -> str:
        # Local llama_cpp (Mistral 7B)
        if self.backend == "llama_cpp" and self.llm is not None:
            try:
                # Usar el modelo Mistral 7B localmente
                response = self.llm(
                    prompt,
                    max_tokens=min(max_tokens, 512),
                    top_p=0.95,
                    top_k=40,
                    temperature=0.7,
                    repeat_penalty=1.1
                )
                if isinstance(response, dict) and "choices" in response:
                    text = response["choices"][0]["text"].strip()
                    return text if text else self._fallback_response(prompt)
                return str(response).strip()
            except Exception as e:
                print(f"‚ùå Error en Mistral local: {e}")
                # Si falla, usa fallback
                pass

        # Remote
        if self.backend == "remote" and self.llm is not None:
            try:
                resp = self.llm.generate(prompt, max_tokens=max_tokens)
                # Si no hay error, devuelve
                if not resp.startswith("["):
                    return resp
            except Exception:
                # Si falla, usa fallback
                pass

        # Fallback behaviour: respuestas inteligentes simuladas
        low = prompt.lower()
        return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt: str) -> str:
        """Generar respuesta de fallback basada en palabras clave."""
        low = prompt.lower()
        
        # Respuestas espec√≠ficas por tema
        if "hola" in low or "buenos" in low or "buen d√≠a" in low:
            return "Hola, soy JARVIS, tu asistente personal. ¬øEn qu√© puedo ayudarte hoy?"
        if "qui√©n eres" in low or "qu√© eres" in low:
            return "Soy JARVIS, tu asistente virtual local. Puedo ejecutar comandos, abrir programas, controlar volumen y enviar correos desde tu computadora. ¬øQu√© necesitas?"
        if "hora" in low or "fecha" in low:
            from datetime import datetime
            ahora = datetime.now().strftime("%H:%M:%S")
            hoy = datetime.now().strftime("%d de %B de %Y")
            return f"Son las {ahora} del {hoy}."
        if "abrir" in low and ("programa" in low or "archivo" in low or "app" in low):
            return "Puedo abrir archivos o programas con `!open <ruta>`. Ejemplo: `!open C:\\Windows\\Notepad.exe`"
        if "ejecutar" in low or "comando" in low or "cmd" in low:
            return "Puedo ejecutar comandos del sistema con `!exec <comando>`. Ejemplo: `!exec dir C:\\ /s`"
        if "volumen" in low or "sonido" in low or "audio" in low:
            return "Puedo ajustar el volumen con `!vol set <0-100>`. Ejemplo: `!vol set 50`"
        if "correo" in low or "email" in low or "enviar" in low:
            return "Puedo enviar correos con `!sendmail`. Necesitar√°s configurar SMTP en `.env`"
        if "memoria" in low or "recordar" in low or "guardar" in low:
            return "Tengo memoria local. Guardo lo que conversamos en `assistant_data/memory.json`"
        if "ayuda" in low or "help" in low or "qu√© puedo hacer" in low:
            return """Puedo hacer lo siguiente:
- Responder preguntas (via IA local o remota)
- Ejecutar comandos: !exec <comando>
- Abrir programas: !open <ruta>
- Controlar volumen: !vol set <0-100>
- Enviar correos: !sendmail
- Recordar conversaciones
- Hablar (TTS) si pyttsx3 est√° disponible

Escribe 'exit' para salir."""
        
        # Respuesta gen√©rica inteligente
        return f"Entendido. Sobre '{low}' ‚Äî necesitar√≠a un modelo de IA remoto activo para responder completamente. Por ahora, puedo ejecutar comandos del sistema, abrir archivos o ayudarte con tareas de productividad. ¬øHay algo espec√≠fico que pueda hacer por ti?"
